\chapter{Concluzie}

\section{Concluzie}
Prin realizarea acestei lucrări am prezentat conceptul de recunoaștere a vorbirii dintr-un videoclip 
integrându-l într-o aplicație web relevantă. Deși scopul proiectului a fost o aplicație web,
librăria Material UI oferă posibilitatea de a fi folosită și pe dispozitivele mobile.
\par
Inițial, mi-am îndreptat atenția către modelul de recunoaștere a vorbirii și generarea subtitrărilor,
dar consider că  integrarea acestuia într-un context concret ilustrează mai bine utilitatea și
importanța acestuia. De aceea, mi-am structurat lucrarea de licență în două părți: partea de
\textbf{Inteligență Artificială}, în care am detaliat arhitectura modelului \textit{wav2vec 2.0},
cum am ales setul de date și cum am antrenat modelul, și arhitectura modelului \textit{BERT}
pentru clasificarea videoclip-urilor, explicată în aceeași manieră, folosită pentru a îmbunătăți
algoritmul de căutare, precum și partea de \textbf{Inginerie Software}, în care am prezentat arhitectura
aplicației web, tehnologiile folosite, structura frontend-ului, backend-ului, bazelor de date,
dar și a serviciilor de deployment și a serviciilor utilitare (Cron Jobs).
\par
Așa cum am menționat în secțiunea \textit{Inginerie Software}, aplicația oferă utilizatorilor
funcționalități precum: autentificare, CRUD pe informațiile proprii, dar si pe videoclip-uri și
interacțiunea cu acestea, căutarea videoclip-urilor după cuvinte cheie și suport pentru recunoașterea
vorbirii, crearea subtitrărilor și clasificarea videoclip-urilor.
\par
În procesul de dezvoltare al aplicației, am gândit întâi funcționalitățile pe care le urmăresc
și apoi am ales tehnologiile potrivite pentru a le implementa. Acest context mi-a oferit 
oportunitatea de a învăța aceste tehonologii și de a le aprofunda în lucrarea de licență.
\par
Am ales deci să utilizez stack-ul \textbf{MERN} (MongoDB, Express.js, React.js, Node.js), folosind 
limbajul TypeScript la care am  adăugat și două servere de Flask în Python. Am ales să fac partea
de antrenare a modelelor tot în Python datorită multitudinii de librării disponibile.
\par
Personal, m-am confruntat cu lipsa subtitrărilor în filme și tutoriale și consider că această
problemă nu este încă rezolvată. Cu cât domeniul de recunoaștere a vorbirii evoluează mai mult
și apar seturi de date pentru mai multe limbi, cu atât mai multe persoane vor beneficia de acces
la materiale video care nu sunt în limba lor maternă. De asemenea, consider că procesul manual
prin care omul adaugă subtitrări este unul consumator de timp și resurse, iar o soluție precum
cea prezentată în această lucrare poate simplifica munca depusă.
\par
În concluzie, lucrarea de licență prezintă un prototip de aplicație web care integrează
recunoașterea vorbirii și generarea subtitrărilor în contextul videoclip-urilor și oferă
utilizatorilor o experiență mai bună în întelegerea conținutului video.

\section{Perspective}
\par
În timpul dezvoltării am observat câteva aspecte care pot fi îmbunătățite pentru o 
performanța mai bună a aplicației. Printre acestea se numără:


\begin{itemize}
    \item \textbf{Suprimarea zgomotului}: am observat că unele videoclip-uri au melodii sau 
    zgomote de fundal care afectează performanța modelului de recunoaștere a vorbirii. Suprimarea
    acestor sunete ar putea îmbunătăți calitatea subtitrărilor.
    \item \textbf{Traducerea subtitrărilor}: m-am concentrat pe limba engleză, având mai multe
    resurse disponibile, dar consider că modelul de recunoaștere a vorbirii poate fi folosit
    și pentru alte limbi, fie prin antrenarea unui model separat, fie prin traducerea subtitrărilor
    generate.
    \item \textbf{Seturi de date mai mari}: am folosit seturi de date mici (\textit{MiniLibriSpeech},
    \textit{Common Voice Delta Segment 16.1}) din lipsa resurselor computaționale de antrenare,
    dar consider că antrenarea pe seturi de date mai mari ar îmbunătăți performanța modelului.
    \item \textbf{Topicuri mai diverse}: videoclip-urile sunt clasificate în 5 categorii
    (\textit{Tech, Sport, Business, Politics, Entertainment}), dar natura materialelor video
    este mult mai diversă de atât. O clasificare mai detaliată ar îmbunătăți experiența
    utilizatorilor.
    \item \textbf{Spell Checker}: deși am îmbunătățit modelul de recunoaștere a vorbirii
    cu n-grame, tot mai apar cuvinte greșite. Un spell checker ar putea corecta aceste greșeli.
    \item \textbf{Suport pentru subtitrări}: librăria \textit{transformers} de pe Huggingface nu oferă
    suport pentru generarea subtitrărilor și a trebuit să prelucrez separat momentele de timp
    ale cuvintelor și să le sincronizez cu videoclip-ul. În implementarea mea am ales să grupez
    câte 7 cuvinte într-o subtitrare, dar în mod normal, această valoare ar trebui să fie ajustată
    la viteza de vorbire.
    \item \textbf{Procesarea paralelă a secvențelor audio}: după cum am menționat mai sus,
    fiecare secvență audio este împărțită în bucăți de 30 de secunde, iar aceste bucăți sunt
    procesate secvențial. Nefiind dependente între ele, pot fi procesate în paralel.
    \item \textbf{Load Balancing}: fiind un prototip, am folosit un singur server pentru toate
    serviciile, dar într-un mediu de producție, arhitecturi precum \textit{nginx} sau \textit{Kubernetes}
    sunt vitale pentru existența aplicației.
    \item \textbf{Replici pentru bazele de date}: în aceeași manieră ca la punctul anterior, am folosit
    o singură bază de date MongoDB și Elasticsearch, dar în producție, arhitecturi de tip 
    \textit{Master-Slave} sau \textit{Sharding} îmbunătățesc viteza de răspuns.
    \item \textbf{Autentificare sigură}: am folosit autentificare cu token JWT, dar în producție,
    servicii de autentificare precum \textit{Auth0} sau \textit{Firebase} sunt mai sigure.
\end{itemize}

\par
Și lista poate continua. Procesul de dezvoltare adoptat a fost unul concentrat pe finalizarea
funcționalităților promise, dar intrând în detalii, sunt multe aspecte care pot fi îmbunătățite.
