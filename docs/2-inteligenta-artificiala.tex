\chapter{Inteligență artificială}

\section{Recunoașterea vorbirii}
Pentru a putea recunoaște vorbirea dintr-un videoclip, am ales să folosesc arhitectura
\textit{wav2vec 2.0} \cite{wav2vec2} dezvoltată de Facebook AI Research. Am folosit 
atât modelul \textit{facebook/wav2vec2-base-960h} antrenat pe setul de date 
\textit{LibriSpeech} \cite{librispeech}, cât și modelul preantrenat
\textit{facebook/wav2vec2-base} pe care am continuat să-l antrenez pe seturile
de date \textit{Mini LibriSpeech} (subset din LibriSpeech) și
\textit{Common Voice Delta Segment 16.1} (subset din Common Voice) \cite{commonvoice}.
\par

\subsection{Arhitectura modelului}
Modelul \textit{wav2vec 2.0} este un model de învățare profundă alcătuit din 4
componente principale: Latent Feature Encoder (Convolutional Network), Context 
Network (Transformer Encoder), Quantization Module (Gumbel Softmax) și 
Contrastive Loss. \ref{fig:wav2vec2-architecture}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{wav2vec2-architecture.png}
    \caption{Arhitectura modelului \textit{wav2vec 2.0} \protect\footnotemark[1]}
    \label{fig:wav2vec2-architecture}
\end{figure}

\subsubsection{Latent Feature Encoder}
\vspace{3em}
Componenta Latent Feature Encoder este o rețea convoluțională care primește ca 
intrare un semnal audio și aplică o serie de operații de convoluție, normalizare
și activări GELU pentru a extrage caracteristici latente din semnalul audio.
\ref{fig:latent-feature-encoder}

\vspace{3em}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{wav2vec2-feature-encoder.png}
    \caption{Arhitectura componentei Latent Feature Encoder \protect\footnotemark[1]}
    \label{fig:latent-feature-encoder}
\end{figure}


\vspace{3em}

\subsubsection{Context Network}
\vspace{1em}
Componenta Context Network este un encoder de tip Transformer care primește ca
intrare caracteristicile latente extrase de componenta Latent Feature Encoder și
le procesează pentru a obține o reprezentare contextuală a semnalului audio. Aducând
aminte de arhitectura modelului anterior \textit{wav2vec} \cite{wav2vec}, care folosea
tot o rețea convoluțională la acest pas, ar părea că se aseamană cu componenta anterioară.
Diferența constă în faptul că Latent Feature Encoder urmărește să reducă dimensiunea 
semnalului audio, în timp ce Context Network urmărește să înțeleagă un context mai larg
al semnalului audio. \ref{fig:wav2vec2-context-network}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{wav2vec2-context-network.png}
    \caption{Arhitectura componentei Context Network \protect\footnotemark[1]}
    \label{fig:wav2vec2-context-network}
\end{figure}

\vspace{3em}

\subsubsection{Quantization Module} 
Deoarece modelul \textit{wav2vec 2.0} folosește pentru partea de Context Network un encoder de tip
Transformer, ne confruntăm cu problema structurii continue a semnalului audio. Limbajul scris poate
fi discretizat într-un set finit de simboluri, în timp ce semnalul audio nu permite în mod direct
acest lucru. Astfel, modelul \textit{wav2vec 2.0} folosește un modul de cuantizare care învață automat
unități de vorbire din semnalul audio. Intuitiv, se încearcă găsirea unor sunete fonetice 
finite și reprezentative pentru ieșirile din Latent Feature Encoder. De asemenea, se aplică 
funcția Gumbel Softmax \cite{gumbel-softmax}, funcție diferențiabilă care permite antrenarea modelului
prin backpropagation. \ref{fig:wav2vec2-quantization-module}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{wav2vec2-quantization-module.png}
    \caption{Arhitectura componentei Quantization Module \protect\footnotemark[1]}
    \label{fig:wav2vec2-quantization-module}
\end{figure}

\subsubsection{Contrastive Loss}
Pentru antrenarea modelului folosește o mască care ascunde ~50\% din vectorii proiectați din spațiul
latent înainte să fie trecuți prin Context Network. Acest lucru forțează modelul să învețe
reprezentări între vectorii proiectați și vectorii ascunși. Pentru fiecare poziție mascată, se
aleg uniform aleator 100 de exemple negative de la alte poziții și se compară similaritatea cosinus
între vectorul proiectat și vectorii aleși. Astfel, funcția de pierdere contrastivă încurajează
similaritatea cu exemplele true positive și penalizează similaritatea cu exemplele false positive.

\begin{figure}[h]
    \centering 
    \includegraphics[width=0.8\textwidth]{wav2vec2-contrastive-loss.png}
    \caption{Arhitectura componentei Contrastive Loss \protect\footnotemark[1]}
    \label{fig:wav2vec2-contrastive-loss}
\end{figure}

\footnotetext[1]{Imaginile au fost preluate de pe site-ul lui Jonathan Bgn, ``Illustrated Wav2Vec 2.0'', disponibil la: \url{https://jonathanbgn.com/2021/09/30/illustrated-wav2vec-2.html}.}

\subsection{Setul de date}
Modelul oficial a fost preantrenat pe setul de date \textit{LibriSpeech}, iar eu am continuat
antrenarea pe seturile de date \textit{Mini LibriSpeech} și \textit{Common Voice Delta Segment 16.1}.

\subsubsection{Mini-LibriSpeech}
\textit{Mini LibriSpeech} este un subset al setului de date \textit{LibriSpeech} care conține 
aproximativ 2 ore de înregistrări audio la o frecvență de eșantionare de 16 kHz. În medie,
fiecare înregistrare are o durată de 6.72 secunde, cel mai lung audio având o durată de 31.5 secunde.

\subsubsection{Common Voice Delta Segment 16.1}
\textit{Common Voice Delta Segment 16.1} este un subset al setului de date \textit{Common Voice}
care conține aproximativ 2 ore de înregistrări audio la o frecvență de eșantionare de 48 kHz.
A fost nevoie să reduc frecvența de eșantionare la 16 kHz pentru a putea folosi aceste date la
antrenarea modelului. În medie, fiecare înregistrare are o durată de 5.63 secunde, cel mai lung
audio având o durată de 10.47 secunde.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{length_distribution.png}
    \caption{Distribuția duratelor audio-urilor din seturile de date \textit{Mini LibriSpeech} și \textit{Common Voice Delta Segment 16.1}}
    \label{fig:length-distribution}
\end{figure}

\subsubsection{Concluzie}
Menționez aceste detalii deoarece pentru generarea subtitrărilor vom avea nevoie de audio-uri
mult mai lungi decât cele folosite pentru antrenare care nu ar încăpea în memorie. Astfel, va trebui
să folosim o tehnică de segmentare a audio-urilor în bucăți mai mici pentru a putea procesa
audio-urile mai lungi. Mai multe detalii despre această tehnică vor fi prezentate în secțiunea
\textit{Subtitrări}.

\subsection{Antrenarea modelului}
Pentru antrenarea modelului am folosit limbajul de programare Python și biblioteca Hugging Face
care pune la dispoziție o serie de pachete precum \textit{transformers} și \textit{datasets}.
\par
Hiperparametrii folosiți pentru fine-tuning-ul modelului sunt:
\begin{itemize}
    \item \textbf{Learning rate} - pentru a controla cât de mult se modifică gradienții în timpul
    antrenării 
    \item \textbf{Weight decay} - pentru a controla cât de mult se penalizează valorile mari ale
    parametrilor 
    \item \textbf{Warmup steps} - pentru a controla cât de mult se modifică learning rate-ul în
    primele pași ai antrenării
    \item \textbf{Batch size} - câte exemple se procesează în același timp
\end{itemize}

\vspace{0.5em}

\begin{table}[h]
    \centering
    \label{tab:hyperparameters}
    \begin{tabular}{ll}
    \hline
    \textbf{Hiperparametru} & \textbf{Valoare} \\ \hline
    Rata de învățare (\textit{learning rate}) & $1 \times 10^{-4}$ \\
    Descreșterea greutății (\textit{weight decay}) & 0.005 \\
    Pași de încălzire (\textit{warmup steps}) & 1000 \\
    Dimensiunea lotului (\textit{batch size}) & 4 \\ \hline
    \end{tabular}
    \caption{Hiperparametrii folosiți pentru fine-tuning-ul modelului \textit{wav2vec2}}
\end{table}

\par
Am antrenat modelul pe seturile de date \textit{Mini LibriSpeech} și \textit{Common Voice Delta Segment 16.1}
pe o placă grafică NVIDIA Tesla V100 pusă la dispoziție de Google Colab. Am făcut un checkpoint la fiecare
500 de pași pentru a putea monitoriza evoluția modelului. Rezultatele checkpoint-urilor pentru modelul
\textit{facebook/wav2vec2-base} sunt prezentate în cele două tabele de mai jos.


% \begin{figure}[h]
%     \centering
%     \begin{minipage}{.5\textwidth}
%         \centering
%         \captionsetup{justification=centering} 
%         \captionof{table}{Mini LibriSpeech}
%         \label{tab:model-checkpoints1}
%         \begin{tabular}{cccc}
%         \toprule
%         \textbf{Step} & \textbf{Train loss} & \textbf{Val loss} & \textbf{wer(\%)} \\
%         \midrule
%         500  & 3.840 & 3.099 & 1.000 \\
%         1000 & 1.202 & 0.586 & 0.361 \\
%         1500 & 0.360 & 0.352 & 0.265 \\
%         2000 & 0.231 & 0.333 & 0.222 \\
%         2500 & 0.163 & 0.357 & 0.230 \\
%         3000 & 0.136 & 0.331 & 0.199 \\
%         3500 & 0.114 & 0.369 & 0.205 \\
%         4000 & 0.104 & 0.348 & 0.203 \\
%         4500 & 0.094 & 0.335 & 0.177 \\
%         5000 & 0.083 & 0.284 & 0.165 \\
%         5500 & 0.078 & 0.332 & 0.165 \\
%         6000 & 0.072 & 0.356 & 0.174 \\
%         6500 & 0.069 & 0.393 & 0.165 \\
%         7000 & 0.066 & 0.380 & 0.181 \\
%         \bottomrule
%         \end{tabular}
%     \end{minipage}%
%     \begin{minipage}{.5\textwidth}
%         \centering
%         \captionof{table}{Common Voice Delta 16.1}
%         \label{tab:model-checkpoints2}
%         \begin{tabular}{cccc}
%         \toprule
%         \textbf{Step} & \textbf{Train loss} & \textbf{Val loss} & \textbf{wer(\%)} \\
%         \midrule
%         500  & 3.919 & 3.236 & 1.000 \\
%         1000 & 1.287 & 0.570 & 0.398 \\
%         1500 & 0.368 & 0.400 & 0.261 \\
%         2000 & 0.226 & 0.371 & 0.252 \\
%         2500 & 0.166 & 0.402 & 0.239 \\
%         3000 & 0.136 & 0.475 & 0.233 \\
%         3500 & 0.116 & 0.445 & 0.201 \\
%         4000 & 0.097 & 0.448 & 0.196 \\
%         4500 & 0.096 & 0.404 & 0.190 \\
%         5000 & 0.079 & 0.459 & 0.182 \\
%         5500 & 0.080 & 0.400 & 0.182 \\
%         6000 & 0.069 & 0.445 & 0.182 \\
%         6500 & 0.073 & 0.421 & 0.179 \\
%         7000 & 0.064 & 0.440 & 0.182 \\
%         \bottomrule
%         \end{tabular}
%     \end{minipage}
% \end{figure}
    
\begin{figure}[h]
    \centering
    \begin{minipage}{.5\textwidth}
        \centering
        \captionsetup{justification=centering} 
        \captionof{table}{Mini LibriSpeech}
        \label{tab:model-checkpoints1}
        \begin{tabular}{ccc}
        \toprule
        \textbf{Step} & \textbf{Train loss} & \textbf{Val loss} \\
        \midrule
        500  & 3.840 & 3.099 \\
        1000 & 1.202 & 0.586 \\
        1500 & 0.360 & 0.352 \\
        2000 & 0.231 & 0.333 \\
        2500 & 0.163 & 0.357 \\
        3000 & 0.136 & 0.331 \\
        3500 & 0.114 & 0.369 \\
        4000 & 0.104 & 0.348 \\
        4500 & 0.094 & 0.335 \\
        5000 & 0.083 & 0.284 \\
        5500 & 0.078 & 0.332 \\
        6000 & 0.072 & 0.356 \\
        6500 & 0.069 & 0.393 \\
        7000 & 0.066 & 0.380 \\
        \bottomrule
        \end{tabular}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
        \centering
        \captionof{table}{Common Voice Delta 16.1}
        \label{tab:model-checkpoints2}
        \begin{tabular}{ccc}
        \toprule
        \textbf{Step} & \textbf{Train loss} & \textbf{Val loss} \\
        \midrule
        500  & 3.919 & 3.236 \\
        1000 & 1.287 & 0.570 \\
        1500 & 0.368 & 0.400 \\
        2000 & 0.226 & 0.371 \\
        2500 & 0.166 & 0.402 \\
        3000 & 0.136 & 0.475 \\
        3500 & 0.116 & 0.445 \\
        4000 & 0.097 & 0.448 \\
        4500 & 0.096 & 0.404 \\
        5000 & 0.079 & 0.459 \\
        5500 & 0.080 & 0.400 \\
        6000 & 0.069 & 0.445 \\
        6500 & 0.073 & 0.421 \\
        7000 & 0.064 & 0.440 \\
        \bottomrule
        \end{tabular}
    \end{minipage}
\end{figure}

\subsubsection{Note}
Se observă că modelul a început să învețe destul de repede, scăzând pierderea de antrenare
de la 3.840 la 0.066, respectiv de la 3.919 la 0.064 în doar 7000 de pași. Urmărind graficul,
se observă fenomenul de \textit{overfitting} care apare în jurul pașilor 5000-6000, motiv
pentru care am ales să opresc antrenarea la 7000 de pași și sa folosesc checkpoint-ul de
la pasul 5000, respectiv 5500.


% CITESTE INAINTE SA CONTINUI SA SCRII
% wer pare ca nu arata valorile corecte la antrenare
% ai testat in scriptul test.py modelul normal si modelul cu language model
% si ai obtinut valori corecte pentru wer

% - explica cum merge n-gram language model
% - arata imbunatatirea (ruleaza script ul test.py)
% - eventual fa discutie si despre fix spelling

\subsection{Îmbunătățire cu n-gram language model}
Pentru a îmbunătăți recunoașterea vorbirii, am folosit un n-gram language model care 
mărește performanța modelului de la \textbf{wer 4.2\%}  la \textbf{wer 2.9\%}, aducând
o îmbunătățire de \textbf{1.3\%}.

\subsubsection{N-gram Language Model}
Un n-gram language model este un model statistic care estimează, în cazul nostru, probabilitatea
apariției unui caracter având în vedere cele n-1 caractere anterioare. Modelul se bazează pe
ipoteza Markov de ordinul n, conform căreia putem aproxima probabilitatea apariției unui caracter
folosind doar ultimele n caractere. Formula de mai jos ilustreaza această idee.

\begin{equation}
    P(w_1, w_2, \ldots, w_n) = \prod_{i=1}^{m} P(w_i | w_{1}, w_{2}, \ldots, w_{i-1}) \approx \prod_{i=1}^{m} P(w_i | w_{i-(n-1)}, \ldots, w_{i-1})
\end{equation}

\par
Am folosit un context de 5 caractere și am antrenat modelul pe
setul de date \textit{Helsinki-NLP/europarl} \cite{tiedemann-2012-parallel} deoarece conține și
texte în limba engleză și putem avea certitudinea că textele sunt corecte din punct de vedere
gramatical. Cu ajutorul librăriei \textit{KenLM} \cite{heafield-2011-kenlm}, am antrenat modelul
de n-grame și apoi am creat un procesor specific Hugging Face pentru modelele \textit{wav2vec 2.0}.

\par
În mod normal, modelul \textit{wav2vec 2.0} ia argumentul maxim din distribuția de probabilitate
pentru a prezice caracterul următor. În schimb, cu ajutorul n-gram language model, aceste
probabilități sunt alterate pentru a se apropia de limbajul natural. 

\section{Clasificarea videoclipurilor}
% TODO

\subsection{Arhitectura modelului}
% TODO

\subsection{Setul de date}
% TODO

\subsection{Antrenarea modelului}
% TODO

\subsection{Postprocesare}
% TODO

\section{Concluzii}
% TODO
